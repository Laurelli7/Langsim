# VLM Robot Planning Pipeline

This repository contains an end-to-end pipeline for generating multimodal training data for vision-language robot navigation tasks. The system automates the creation of training environments, simulates a "Human Oracle" to guide the robot, and records the robot's planning execution for future Large Language Model (LLM) fine-tuning.

## 1\. Pipeline Overview

The pipeline operates in two distinct phases:

1.  **Offline Scene Baking:** We programmatically generate randomized USD stages containing a TurtleBot4, random cylindrical targets, and ground truth metadata. These scenes are "baked" (dynamic graphs stripped) to ensure deterministic loading during simulation.
2.  **Online Data Generation (The "Game"):** An autonomous loop where two AI Agents interact:
      * **Human Oracle:** Has a top-down "God view" of the map and the target. It gives natural language hints to the robot.
      * **Planner Agent:** Acts as the robot. It sees only the ego-centric camera view and the human's hint. It outputs executable ROS 2 code to move the robot.

## 2\. File Descriptions

### `gen.py`

**The Scene Baker**
This script generates the dataset of environments used for training.

  * **Function:** Loads a base TurtleBot4 USD, spawns cylinders in random locations with random colors, and randomizes the robot's pose.
  * **Key Feature:** It performs a "Snapshot & Strip" operation. It runs Isaac Replicator for one frame to randomize the scene, exports the stage to a new `.usd` file, and then **strips the Replicator/OmniGraph nodes**.
  * **Output:** Static `.usd` files (scenes) and companion `.json` files (ground truth locations/colors).

### `langsim.py`

**The Simulation Loop (Online Generation)**
The main entry point for data collection. It loads a baked scene and orchestrates the interaction between the Agents.

  * **Agents:**
      * **Oracle:** Receives a top-down image with a drawn arrow indicating robot pose. Uses GPT-4o to generate navigation instructions (e.g., "Turn left toward the blue cylinder").
      * **Planner:** Receives the ego-centric robot view and the Oracle's text. Uses GPT-4o to generate **Python ROS 2 code**.
  * **Execution:** The script extracts the Python code generated by the Planner, saves it to a temporary file, and executes it via `subprocess` to actually move the robot in the sim.
  * **Logging:** Saves every interaction (images, prompts, generated code) to `dataset_logs/`.

### `camera.py`

**Sensor Utility: Vision**
A configuration script to attach and initialize the camera sensors on the robot.

  * **Function:** Wraps the USD prim path (e.g., `/World/turtlebot4/oakd_link/Camera`) with Isaac Sim's `Camera` class.
  * **ROS 2 Bridge:** Sets up the Action Graphs required to publish:
      * RGB Images (`/camera_rgb`)
      * Depth Maps (`/camera_depth`)
      * Camera Info (intrinsics)
      * TF (Transforms)

### `lidar.py`

**Sensor Utility: Depth/Scan**
A configuration script to attach and initialize the Lidar sensor.

  * **Function:** Creates a custom OmniGraph to bridge PhysX Lidar data to ROS 2.
  * **ROS 2 Bridge:** Publishes standard `/scan` (LaserScan) messages used for obstacle avoidance and mapping logic.

## 3\. Usage Guide

### Prerequisites

  * **Isaac Sim 4.0+**
  * **ROS 2 Humble/Foxy** installed and sourced in the environment.
  * **OpenAI API Key** (for GPT-4o).

### Step 1: Generate Scenes (Offline)

Run the randomizer to create a batch of 50 static scenarios.

```bash
# Ensure you are in the Isaac Sim python environment
./python.sh gen.py
```

*Output:* Checks `scene_snapshots/` for `scene_0000.usd`, `scene_0000_gt.json`, etc.

### Step 2: Run Data Collection (Online)

Set your environment variables and run the generation loop.

```bash
export OPENAI_API_KEY="sk-..."
export TOPIC_CMD_VEL="/cmd_vel"

# Run the simulation app
./python.sh langsim.py
```

*Output:* The robot will load a scene, ask for help, receive a hint, generate code, and move. Logs are saved to `dataset_logs/`.

## 4\. Dataset Output Format

The `langsim.py` script produces a `log_{id}.json` file for every episode containing the chain-of-thought data required for fine-tuning:

```json
{
  "scene": "0000",
  "dialogue": [
    {
      "round": 0,
      "robot_pos": [1.2, -0.5, 0.0],
      "planner_action": "ask",
      "planner_content": "I see a wall. Where is the green cylinder?",
      "human_hint": "Turn right 90 degrees, it is behind you."
    },
    {
      "round": 1,
      "robot_pos": [1.2, -0.5, 1.57],
      "planner_action": "code",
      "planner_content": "import rclpy... # logic to turn right",
      "result": "Success"
    }
  ]
}
```

## 5\. Future Work

  * **Fine-tuning:** The logs in `dataset_logs` will be used to fine-tune a smaller VLM (e.g., LLaVA or a distilled model) to act as the Planner *without* needing GPT-4o or the Python code generation step (end-to-end control).
  * **RL Integration:** Using the generated code snippets as "expert demonstrations" for Imitation Learning.