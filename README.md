# VLM Robot Planning Pipeline

This repository contains an end-to-end pipeline for generating multimodal training data for vision-language robot navigation tasks. The system automates the creation of training environments, simulates a "Human Oracle" to guide the robot, and records the robot's planning execution for future Large Language Model (LLM) fine-tuning.

## 1\. Pipeline Overview

The pipeline operates in two distinct phases:

1.  **Offline Scene Baking:** We programmatically generate randomized USD stages containing a TurtleBot4, random cylindrical targets, and ground truth metadata. These scenes are "baked" (dynamic graphs stripped) to ensure deterministic loading during simulation.
2.  **Online Data Generation (The "Game"):** An autonomous loop where two AI Agents interact:
      * **Human Oracle:** Has a top-down "God view" of the map and the target. It gives natural language hints to the robot.
      * **Planner Agent:** Acts as the robot. It sees only the ego-centric camera view and the human's hint. It outputs executable ROS 2 code to move the robot.

## 2\. File Descriptions

### `gen.py`

**The Scene Baker**
This script generates the dataset of environments used for training.

  * **Function:** Loads a base TurtleBot4 USD, spawns cylinders in random locations with random colors, and randomizes the robot's pose.
  * **Key Feature:** It performs a "Snapshot & Strip" operation. It runs Isaac Replicator for one frame to randomize the scene, exports the stage to a new `.usd` file, and then **strips the Replicator/OmniGraph nodes**.
  * **Output:** Static `.usd` files (scenes) and companion `.json` files (ground truth locations/colors).

### `langsim.py`

**The Simulation Loop (Online Generation)**
The main entry point for data collection. It loads a baked scene and orchestrates the interaction between the Agents.

  * **Agents:**
      * **Oracle:** Receives a top-down image with a drawn arrow indicating robot pose. Uses GPT-4o to generate navigation instructions (e.g., "Turn left toward the blue cylinder").
      * **Planner:** Receives the ego-centric robot view and the Oracle's text. Uses GPT-4o to generate **Python ROS 2 code**.
  * **Execution:** The script extracts the Python code generated by the Planner, saves it to a temporary file, and executes it via `subprocess` to actually move the robot in the sim.
  * **Logging:** Saves every interaction (images, prompts, generated code) to `dataset_logs/`.

### `camera.py`

**Sensor Utility: Vision**
A configuration script to attach and initialize the camera sensors on the robot.

  * **Function:** Wraps the USD prim path (e.g., `/World/turtlebot4/oakd_link/Camera`) with Isaac Sim's `Camera` class.
  * **ROS 2 Bridge:** Sets up the Action Graphs required to publish:
      * RGB Images (`/camera_rgb`)
      * Depth Maps (`/camera_depth`)
      * Camera Info (intrinsics)
      * TF (Transforms)

### `lidar.py`

**Sensor Utility: Depth/Scan**
A configuration script to attach and initialize the Lidar sensor.

  * **Function:** Creates a custom OmniGraph to bridge PhysX Lidar data to ROS 2.
  * **ROS 2 Bridge:** Publishes standard `/scan` (LaserScan) messages used for obstacle avoidance and mapping logic.

## 3\. Usage Guide

### Prerequisites

  * **Isaac Sim 4.0+**
  * **ROS 2 Humble/Foxy** installed and sourced in the environment.
  * **OpenAI API Key** (for GPT-4o).

### Step 1: Generate Scenes (Offline)

Run the randomizer to create a batch of 50 static scenarios.

```bash
# Ensure you are in the Isaac Sim python environment
./python.sh gen.py
```

*Output:* Checks `scene_snapshots/` for `scene_0000.usd`, `scene_0000_gt.json`, etc.

### Step 2: Run Data Collection (Online)

Set your environment variables and run the generation loop.

```bash
export OPENAI_API_KEY="sk-..."
export TOPIC_CMD_VEL="/cmd_vel"

# Run the simulation app
./python.sh langsim.py
```

*Output:* The robot will load a scene, ask for help, receive a hint, generate code, and move. Logs are saved to `dataset_logs/`.

## 4\. Dataset Output Format

The `langsim.py` script produces a `log_{id}.json` file for every episode containing the chain-of-thought data required for fine-tuning:

```json
{
  "scene": "0000",
  "dialogue": [
    {
      "round": 0,
      "robot_pos": [1.2, -0.5, 0.0],
      "planner_action": "ask",
      "planner_content": "I see a wall. Where is the green cylinder?",
      "human_hint": "Turn right 90 degrees, it is behind you."
    },
    {
      "round": 1,
      "robot_pos": [1.2, -0.5, 1.57],
      "planner_action": "code",
      "planner_content": "import rclpy... # logic to turn right",
      "result": "Success"
    }
  ]
}
```

## 5\. Future Work

  * **Fine-tuning:** The logs in `dataset_logs` will be used to fine-tune a smaller VLM (e.g., LLaVA or a distilled model) to act as the Planner *without* needing GPT-4o or the Python code generation step (end-to-end control).
  * **RL Integration:** Using the generated code snippets as "expert demonstrations" for Imitation Learning.


## 6\. Final Writeup
## Project Description
Langsim is a human-VLM (Vision-Language Model) collaboration framework designed to enable natural language planning for ROS 2 robots. The goal of this project is to lower the barrier for human-robot interaction by allowing users to control a robot, such as a TurtleBot 4, using natural language or voice commands. This project is particularly interesting because it leverages the code generation and transformation capabilities of Multimodal LLMs to analyze visual and text inputs and dynamically plan tasks, rather than relying on static action primitives.

We successfully implemented a system where a robot can interpret a high-level command like *"Find and move to the blue cylinder."* If the robot fails (e.g., returns `TASK_RESULT:FAIL`), the framework supports human-in-the-loop guidance. A user can provide a corrective prompt, such as *"If you move to the pink cylinder first... you should be able to find and reach the blue cylinder,"* which allows the robot to update its plan and succeed.

### Main Components
* **Human Supervisor (Oracle):** Provides natural language guidance, queries, and ground truth information (e.g., map, target).
* **VLM-Based Planner Agent:** Acts as the reasoning layer. It decides whether to "Ask" for help or "Code" a solution. It integrates visual perception (egocentric RGB images) and robot state to generate Python code.
* **ROS 2 Execution Environment:** The generated code is executed on the robot (TurtleBot 4), sending commands to topics like `/cmd_vel`.
* **Simulation & Offline Learning:** NVIDIA Isaac Sim and Replicator were used to generate synthetic datasets of Python scripts to fine-tune the model for spatial reasoning and interactiveness.

## System Architecture
The Langsim architecture operates as a four-stage inference pipeline:

1.  **Voice Input:** The user speaks a natural language command via a microphone.
2.  **VLM Planning:** A fine-tuned VLM (e.g., Qwen-VL or Mistral 3) analyzes the user's intent and the current visual state from the robot's camera. It acts as a high-level planner, deciding if it has enough information to act or if it needs to query the human.
3.  **Code Generation:** The model generates specific ROS 2 Python nodes to execute the identified subtask. For example, to find an object, it might generate a class like `FindCylinder(Node)` that subscribes to the camera and scan data.
4.  **Action & Feedback:** The TurtleBot executes the generated plan in the environment. The generated code is designed to be deterministic, printing results like `TASK_RESULT:SUCCESS` or `TASK_RESULT:FAIL`. If a failure occurs, the robot prompts the human for guidance, closing the loop.

## Challenges
We faced significant difficulties with the **NVIDIA Isaac Sim** and **Replicator** workflows. The Omniverse API is deprecated, and many community resources (such as documentation for camera projection matrices) have not been updated, making simulation setup harder than anticipated.

Additionally, **human-in-the-loop data generation** proved to be slow and complicated, as the requirement for constant human presence bottlenecked the data collection process.

## Future Work
* Assess how human emotions affect model performance and exploratory incentives during collaboration.
* Generalize this framework to more complicated tasks, such as multi-room navigation (logistics) and pick-and-place operations for medical or surgical applications.
* Investigate the possibilities of a collaborative agentic framework in VLM robot manipulation, which remains a key research question.

## Takeaways
* **Human Guidance is Critical:** LLMs can "hallucinate" or fail on spatial tasks, but simple human hints can effectively unblock them.
* **Simulation vs. Real World:** Transferring simulation results to a real robot remains a major risk and challenge due to physical discrepancies.
* **Code Gen over Primitives:** Generating dynamic ROS 2 code offers greater flexibility than mapping to fixed action primitives, though it requires robust safety checks.
