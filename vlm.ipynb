{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c38158",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71a3ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 log files. Processing...\n",
      "Successfully processed 20 dialogues.\n",
      "Saved to vlm_sft_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "LOG_DIR = \"./dataset_logs\"\n",
    "OUTPUT_FILE = \"vlm_sft_dataset.json\"\n",
    "\n",
    "\n",
    "def create_sft_dataset(log_dir):\n",
    "    sft_data = []\n",
    "\n",
    "    # Find all JSON log files\n",
    "    json_files = glob.glob(os.path.join(log_dir, \"log_*.json\"))\n",
    "\n",
    "    print(f\"Found {len(json_files)} log files. Processing...\")\n",
    "\n",
    "    for json_path in json_files:\n",
    "        try:\n",
    "            with open(json_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            scene_id = data.get(\"scene\", \"\")\n",
    "            run_idx = data.get(\"run_idx\", 0)\n",
    "            raw_dialogue = data.get(\"dialogue\", [])\n",
    "\n",
    "            # Construct the conversation list for this specific run\n",
    "            conversation = []\n",
    "\n",
    "            # We track the round index to find the matching image (r0, r1, r2...)\n",
    "            current_round_idx = 0\n",
    "\n",
    "            for idx, message in enumerate(raw_dialogue):\n",
    "                role = message.get(\"role\")\n",
    "                content = message.get(\"content\")\n",
    "\n",
    "                if role == \"system\":\n",
    "                    # System prompt stays text-only\n",
    "                    conversation.append({\"role\": \"system\", \"content\": content})\n",
    "\n",
    "                    img_filename = (\n",
    "                        f\"{scene_id}_run{run_idx}_r{current_round_idx}_ego.jpg\"\n",
    "                    )\n",
    "                    img_path = os.path.join(log_dir, img_filename)\n",
    "                    conversation.append(\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"image\", \"image\": img_path},\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": \"This is the current camera view.\",\n",
    "                                },\n",
    "                            ],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    current_round_idx += 1\n",
    "\n",
    "                elif role == \"user\":\n",
    "                    # This is where we inject the image\n",
    "                    # e.g., \"0010_run0_r0_ego.jpg\"\n",
    "                    img_filename = (\n",
    "                        f\"{scene_id}_run{run_idx}_r{current_round_idx}_ego.jpg\"\n",
    "                    )\n",
    "                    img_path = os.path.join(log_dir, img_filename)\n",
    "\n",
    "                    # Check if the image actually exists for this round\n",
    "                    if os.path.exists(img_path):\n",
    "                        # Create the multimodal user message\n",
    "                        user_turn = {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"image\", \"image\": img_path},\n",
    "                                {\"type\": \"text\", \"text\": content},\n",
    "                            ],\n",
    "                        }\n",
    "                        # Increment round index only after we consume an image for a user turn\n",
    "                        current_round_idx += 1\n",
    "                    else:\n",
    "                        # Fallback if image is missing: keep text only\n",
    "                        user_turn = {\"role\": \"user\", \"content\": content}\n",
    "\n",
    "                    conversation.append(user_turn)\n",
    "\n",
    "                elif role == \"assistant\":\n",
    "                    # Assistant replies are usually text/code\n",
    "                    conversation.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "            # Add this conversation to the master list\n",
    "            sft_data.append({\"dialogue\": conversation})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_path}: {e}\")\n",
    "\n",
    "    # Save the final dataset\n",
    "    with open(OUTPUT_FILE, \"w\") as f:\n",
    "        json.dump(sft_data, f, indent=4)\n",
    "\n",
    "    print(f\"Successfully processed {len(sft_data)} dialogues.\")\n",
    "    print(f\"Saved to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_sft_dataset(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U peft bitsandbytes accelerate datasets trl qwen-vl-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe773c6",
   "metadata": {},
   "source": [
    "## Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41128c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fedf51993224433987b11d4d278a6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    Qwen3VLForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "MODEL_ID = \"/project/jevans/tzhang3/models/Qwen3-VL-8B-Instruct\"\n",
    "\n",
    "# Configure 4-bit quantization to fit in VRAM\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load Processor\n",
    "# min_pixels/max_pixels control the resolution.\n",
    "# Reducing max_pixels saves VRAM significantly.\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID, min_pixels=256 * 28 * 28, max_pixels=640 * 28 * 28\n",
    ")\n",
    "\n",
    "# Load Model\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f5d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 43,646,976 || all params: 8,810,770,672 || trainable%: 0.4954\n"
     ]
    }
   ],
   "source": [
    "# @title 4. Apply LoRA Adapters\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[],  # We don't need to save embed_tokens for code generation\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc990393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 20 samples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "class RobotDataset(TorchDataset):\n",
    "    def __init__(self, data_path, processor, image_base_dir):\n",
    "        self.processor = processor\n",
    "        self.image_base_dir = image_base_dir\n",
    "        \n",
    "        # Rename 'self.data' to 'self.examples' to avoid property conflicts\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.examples = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        messages = example[\"dialogue\"]\n",
    "\n",
    "        # Pre-process: Fix Paths and Types\n",
    "        cleaned_messages = []\n",
    "        for msg in messages:\n",
    "            new_msg = msg.copy()\n",
    "            \n",
    "            # Type Fix\n",
    "            if isinstance(new_msg['content'], str):\n",
    "                new_msg['content'] = [{\"type\": \"text\", \"text\": new_msg['content']}]\n",
    "            \n",
    "            cleaned_messages.append(new_msg)\n",
    "\n",
    "        # Apply Template & Tokenize\n",
    "        text = self.processor.apply_chat_template(cleaned_messages, tokenize=False, add_generation_prompt=False)\n",
    "        image_inputs, video_inputs = process_vision_info(cleaned_messages)\n",
    "        \n",
    "        # INCREASED MAX_LENGTH to prevent truncation of images\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Standard squeeze for input_ids (1, seq_len) -> (seq_len)\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Masking Logic\n",
    "        labels[:] = -100 \n",
    "        \n",
    "        assistant_header = \"<|im_start|>assistant\\n\"\n",
    "        header_ids = self.processor.tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "        end_token_id = self.processor.tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)[0]\n",
    "        \n",
    "        input_ids_list = input_ids.tolist()\n",
    "        seq_len = len(input_ids_list)\n",
    "        header_len = len(header_ids)\n",
    "        \n",
    "        i = 0\n",
    "        while i < seq_len - header_len:\n",
    "            if input_ids_list[i : i + header_len] == header_ids:\n",
    "                content_start = i + header_len\n",
    "                content_end = -1\n",
    "                for j in range(content_start, seq_len):\n",
    "                    if input_ids_list[j] == end_token_id:\n",
    "                        content_end = j\n",
    "                        break\n",
    "                \n",
    "                if content_end != -1:\n",
    "                    labels[content_start : content_end + 1] = input_ids[content_start : content_end + 1]\n",
    "                    i = content_end\n",
    "                else:\n",
    "                    labels[content_start:] = input_ids[content_start:]\n",
    "                    break\n",
    "            i += 1\n",
    "\n",
    "        result = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "        if \"pixel_values\" in inputs:\n",
    "            result[\"pixel_values\"] = inputs[\"pixel_values\"] # Usually 1D flattened, keep as is or check\n",
    "            \n",
    "            # Handle Grid THW: (N_images, 3)\n",
    "            grid = inputs[\"image_grid_thw\"]\n",
    "            \n",
    "            # If it comes out as (1, N, 3), squeeze the batch dim -> (N, 3)\n",
    "            if grid.dim() == 3:\n",
    "                result[\"image_grid_thw\"] = grid.squeeze(0)\n",
    "            # If it comes out as (1, 3) or (N, 3), KEEP IT. \n",
    "            # DO NOT SQUEEZE if dim() == 2, otherwise it becomes (3,) -> CRASH\n",
    "            else:\n",
    "                result[\"image_grid_thw\"] = grid\n",
    "            \n",
    "        return result\n",
    "\n",
    "# Instantiate Dataset\n",
    "train_dataset = RobotDataset(\"vlm_sft_dataset.json\", processor, \"dataset_logs\")\n",
    "print(f\"Dataset loaded with {len(train_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0ccf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class Qwen3VLDataCollator:\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
    "        \n",
    "        pixel_values = [f[\"pixel_values\"] for f in features if \"pixel_values\" in f]\n",
    "        image_grid_thw = [f[\"image_grid_thw\"] for f in features if \"image_grid_thw\" in f]\n",
    "\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "        if pixel_values:\n",
    "            batch[\"pixel_values\"] = torch.cat(pixel_values, dim=0)\n",
    "            batch[\"image_grid_thw\"] = torch.cat(image_grid_thw, dim=0)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a6973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tzhang3/envs/mineru/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tzhang3/envs/mineru/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 39:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.547400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.489100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.546500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.459300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tzhang3/envs/mineru/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tzhang3/envs/mineru/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/tzhang3/envs/mineru/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tzhang3/envs/mineru/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.5629600346088409, metrics={'train_runtime': 2553.5841, 'train_samples_per_second': 0.023, 'train_steps_per_second': 0.006, 'total_flos': 4.715013086208e+16, 'train_loss': 0.5629600346088409, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen3-vl-robot-planner\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=False, \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=Qwen3VLDataCollator(),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd47687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
